{"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4418575,"sourceType":"datasetVersion","datasetId":2588484}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/sevdanuropur/nlp-sentiment-analysis?scriptVersionId=201460114\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# Sentiment Analysis for Amazon Reviews","metadata":{}},{"cell_type":"markdown","source":"**CONTENT**\n\n* Aim of the Project \n* Dataset \n* IMPORT LIBRARIRES\n* LOAD DATA SET\n* Standardization\n* Punctuation Removal\n* Number Removal\n* Stopwords\n* Rare Words\n* Lemmatization\n* Visualization\n* Wordcloud \n* Sentiment Analysis\n* Modelling \n* CountVectorize()\n* Logistig Regression with CountVectorize()\n* TF-IDF\n* Logistig Regression with TF-IDF\n* TF-IDF- Ngram \n* Logistig Regression with Ngram \n* Random Forest \n* Result \n* Conclusion","metadata":{}},{"cell_type":"markdown","source":"**Aim of the Project**\n\nKozmos, a company that sells home textiles and casual wear on Amazon, aims to boost sales by analyzing customer reviews and improving its products based on recurring complaints. To achieve this, sentiment analysis will be performed on the reviews to categorize them, and a classification model will be built using the tagged data to further support decision-making.","metadata":{}},{"cell_type":"markdown","source":"**DATASET**\n\nThis dataset contains 4 features and consists of 5611 observations with a size of 489 KB. It includes customer reviews for a specific product group along with relevant details:\n\n* Star: The number of stars given to the product.\n* Helpful: The number of people who found the review helpful.\n* Title: The title or short description of the review.\n* Review: The full content of the review.","metadata":{}},{"cell_type":"markdown","source":"**IMPORT LIBRARIES**","metadata":{}},{"cell_type":"code","source":"from warnings import filterwarnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns  \n\n#### NLTK#### \n\nimport nltk \nfrom nltk.corpus import stopwords \nfrom nltk.stem import WordNetLemmatizer \nfrom wordcloud import WordCloud\nfrom nltk.sentiment import SentimentIntensityAnalyzer \nfrom textblob import Word, TextBlob\n\n## Sklearn \nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.feature_extraction.text import TfidfVectorizer \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, classification_report\n\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.float_format', lambda x: '%.2f' % x)","metadata":{"execution":{"iopub.execute_input":"2024-10-12T21:01:20.974134Z","iopub.status.busy":"2024-10-12T21:01:20.972692Z","iopub.status.idle":"2024-10-12T21:01:20.984213Z","shell.execute_reply":"2024-10-12T21:01:20.982961Z","shell.execute_reply.started":"2024-10-12T21:01:20.974071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Load Dataset**","metadata":{}},{"cell_type":"code","source":"df=pd.read_excel(\"/kaggle/input/amazon-reviews/amazon.xlsx\") \ndf.head()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:16:04.218484Z","iopub.status.busy":"2024-10-12T20:16:04.218046Z","iopub.status.idle":"2024-10-12T20:16:05.240994Z","shell.execute_reply":"2024-10-12T20:16:05.239413Z","shell.execute_reply.started":"2024-10-12T20:16:04.21844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_df(dataframe, head=5):\n    \"\"\"\n    Provides a general overview of the dataset.\n    \n    Parameters:\n    dataframe: The dataframe to be read and analyzed.\n    head (int): The number of rows to display initially. The default value is 5.\n    \"\"\"\n    print(\"The first {} rows of the DataFrame:\".format(head))\n    display(dataframe.head(head))\n    print(\"---------------------------------\")\n    print(\"Shape information of the DataFrame:\")\n    display(dataframe.shape)\n    print(\"---------------------------------\")\n    print(\"General information about the DataFrame:\")\n    dataframe.info()\n    print(\"---------------------------------\")\n    print(\"Missing value information in the DataFrame:\")\n    display(dataframe.isnull().sum())\n    print(\"---------------------------------\")\n    return True\n\ncheck_df(df)\n","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:08:16.080455Z","iopub.status.busy":"2024-10-12T20:08:16.080063Z","iopub.status.idle":"2024-10-12T20:08:16.119336Z","shell.execute_reply":"2024-10-12T20:08:16.11809Z","shell.execute_reply.started":"2024-10-12T20:08:16.080415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Let's look at the number of stars missing in Review u\ndf[df[\"Review\"].isnull()][\"Star\"].value_counts()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:09:31.600834Z","iopub.status.busy":"2024-10-12T20:09:31.60039Z","iopub.status.idle":"2024-10-12T20:09:31.611825Z","shell.execute_reply":"2024-10-12T20:09:31.610657Z","shell.execute_reply.started":"2024-10-12T20:09:31.60079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[(df[\"Title\"].isnull()) & (df[\"Review\"].isnull())].shape","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:08:17.178628Z","iopub.status.busy":"2024-10-12T20:08:17.178217Z","iopub.status.idle":"2024-10-12T20:08:17.189547Z","shell.execute_reply":"2024-10-12T20:08:17.188285Z","shell.execute_reply.started":"2024-10-12T20:08:17.17859Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Lets drop the null values of Review features \n\ndf= df[~df[\"Review\"].isnull()]","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:16:10.292563Z","iopub.status.busy":"2024-10-12T20:16:10.292001Z","iopub.status.idle":"2024-10-12T20:16:10.301564Z","shell.execute_reply":"2024-10-12T20:16:10.299968Z","shell.execute_reply.started":"2024-10-12T20:16:10.292498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.isnull().sum()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:08:17.977466Z","iopub.status.busy":"2024-10-12T20:08:17.977059Z","iopub.status.idle":"2024-10-12T20:08:17.988511Z","shell.execute_reply":"2024-10-12T20:08:17.987359Z","shell.execute_reply.started":"2024-10-12T20:08:17.977428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Review\"].head(10)","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:08:18.554359Z","iopub.status.busy":"2024-10-12T20:08:18.553908Z","iopub.status.idle":"2024-10-12T20:08:18.563924Z","shell.execute_reply":"2024-10-12T20:08:18.562574Z","shell.execute_reply.started":"2024-10-12T20:08:18.554314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"star_counts = df[\"Star\"].value_counts()\n\nplt.figure(figsize=(6, 6))\nplt.pie(star_counts, labels=star_counts.index, autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff','#99ff99'])\nplt.title(\"Star Column Distribution\")\nplt.axis('equal')  \nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:08:18.83291Z","iopub.status.busy":"2024-10-12T20:08:18.832441Z","iopub.status.idle":"2024-10-12T20:08:19.041996Z","shell.execute_reply":"2024-10-12T20:08:19.040394Z","shell.execute_reply.started":"2024-10-12T20:08:18.832852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Standardization**\n\n* We need to convert all letters to lowercase because converting words that start with uppercase letters but have the same meaning helps machines understand them better. By standardizing to lowercase format, we reduce the complexity of the text and ensure that similar words are treated as the same, improving the accuracy of text processing and analysis.","metadata":{}},{"cell_type":"code","source":"df[\"Review\"]= df[\"Review\"].str.lower()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:16:17.07179Z","iopub.status.busy":"2024-10-12T20:16:17.071333Z","iopub.status.idle":"2024-10-12T20:16:17.08381Z","shell.execute_reply":"2024-10-12T20:16:17.082494Z","shell.execute_reply.started":"2024-10-12T20:16:17.071745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Punctuation Removal**\n\n* Removing punctuation marks is important in NLP as they can disrupt word analysis. By eliminating punctuation, we clean the text, allowing algorithms to focus on the content and improve the accuracy of tasks like sentiment analysis.","metadata":{}},{"cell_type":"code","source":"df[\"Review\"]= df[\"Review\"].str.replace('[^\\w\\s]', '')","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:16:19.307717Z","iopub.status.busy":"2024-10-12T20:16:19.307275Z","iopub.status.idle":"2024-10-12T20:16:19.317937Z","shell.execute_reply":"2024-10-12T20:16:19.316649Z","shell.execute_reply.started":"2024-10-12T20:16:19.307675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Number Removal**\n\n* Removing numbers from text is essential in NLP because they often do not contribute to the meaning in sentiment analysis or text classification. ","metadata":{}},{"cell_type":"code","source":"df[\"Review\"]= df[\"Review\"].str.replace('\\d',\"\")","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:16:39.321892Z","iopub.status.busy":"2024-10-12T20:16:39.32143Z","iopub.status.idle":"2024-10-12T20:16:39.332023Z","shell.execute_reply":"2024-10-12T20:16:39.330844Z","shell.execute_reply.started":"2024-10-12T20:16:39.321835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Stopwords**\n\n* We remove stopwords, which are common words that carry little meaning, from the text. These words often include articles, prepositions, and conjunctions that do not add significant value to the analysis, allowing the model to focus on more meaningful terms. \n","metadata":{}},{"cell_type":"code","source":"sw = stopwords.words('english')","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:16:41.45177Z","iopub.status.busy":"2024-10-12T20:16:41.451319Z","iopub.status.idle":"2024-10-12T20:16:41.458129Z","shell.execute_reply":"2024-10-12T20:16:41.456956Z","shell.execute_reply.started":"2024-10-12T20:16:41.451725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# these are the words that have little meaning in a sentence\nsw[:10]","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:16:43.888921Z","iopub.status.busy":"2024-10-12T20:16:43.888496Z","iopub.status.idle":"2024-10-12T20:16:43.896859Z","shell.execute_reply":"2024-10-12T20:16:43.895506Z","shell.execute_reply.started":"2024-10-12T20:16:43.888862Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This apply function eliminates stopwords from each row in the 'Review' column \ndf['Review']= df['Review'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in sw))","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:16:46.011957Z","iopub.status.busy":"2024-10-12T20:16:46.011504Z","iopub.status.idle":"2024-10-12T20:16:46.324953Z","shell.execute_reply":"2024-10-12T20:16:46.323688Z","shell.execute_reply.started":"2024-10-12T20:16:46.011913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Rare Words**\n\n* This step involves removing words that occur fewer than 1 times in the dataset. Rare words can introduce noise and don't contribute significantly to sentiment analysis. By eliminating them, we focus on more relevant terms, improving the dataset's quality for model training.","metadata":{}},{"cell_type":"code","source":"# make a new series that counts the occurrences of each word.\nnew_df = pd.Series(' '.join(df['Review']).split()).value_counts()\n\n# select words lest than 1 \nrare_words = new_df[new_df <= 1] \n\n# elimate these words from each row \ndf['Review'] = df['Review'].apply(lambda x: \" \".join(x for x in x.split() if x not in rare_words))","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:16:48.768632Z","iopub.status.busy":"2024-10-12T20:16:48.768181Z","iopub.status.idle":"2024-10-12T20:16:49.071355Z","shell.execute_reply":"2024-10-12T20:16:49.070201Z","shell.execute_reply.started":"2024-10-12T20:16:48.768588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Lemmatization:**\n\n* Lemmatization is the process of reducing words to their base or root form, known as the lemma. For example, the words \"glasses,\" \"eye,\" and \"eyes\" all reduce to the root word \"eye.\" This helps standardize words, making text analysis more effective by treating different forms of a word as the same.","metadata":{}},{"cell_type":"code","source":"lemmatizer= WordNetLemmatizer() ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lemmatize(text):\n    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Review\"]= df[\"Review\"].apply(lambda x : lemmatize(x))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Visualization**","metadata":{}},{"cell_type":"code","source":"count_df= pd.Series(\" \".join(df[\"Review\"]).split()).value_counts()\n\ncount_df= count_df.reset_index()\n\ncount_df.columns=[\"words\",\"tf\"]","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:19:39.340417Z","iopub.status.busy":"2024-10-12T20:19:39.339948Z","iopub.status.idle":"2024-10-12T20:19:39.384474Z","shell.execute_reply":"2024-10-12T20:19:39.383491Z","shell.execute_reply.started":"2024-10-12T20:19:39.340374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"count_df[count_df[\"tf\"]>500]","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:19:41.105115Z","iopub.status.busy":"2024-10-12T20:19:41.104658Z","iopub.status.idle":"2024-10-12T20:19:41.118792Z","shell.execute_reply":"2024-10-12T20:19:41.117428Z","shell.execute_reply.started":"2024-10-12T20:19:41.105071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6)) \n\nsns.barplot(data=count_df[count_df[\"tf\"] > 500], x=\"words\", y=\"tf\", palette=\"viridis\") \n\nplt.xticks(rotation=45, ha='right', fontsize=10)  # Rotate and align the labels\n\n# Add title to axes\n\nplt.xlabel(\"Words\", fontsize=12, labelpad=10)  # X-axis title and label padding\n\nplt.ylabel(\"Term Frequency (tf)\", fontsize=12, labelpad=10)  # Y-axis title\n\n# Graph title\n\nplt.title(\"Term Frequency for Words with tf > 500\", fontsize=14, pad=15)\n\n# Graph borders\nsns.despine()  # We can remove the borders around the graph\n# Show the graph\n\nplt.tight_layout()  # Ensures all elements are displayed neatly\n\nplt.show()\n\n","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:19:46.82767Z","iopub.status.busy":"2024-10-12T20:19:46.827245Z","iopub.status.idle":"2024-10-12T20:19:47.284101Z","shell.execute_reply":"2024-10-12T20:19:47.282902Z","shell.execute_reply.started":"2024-10-12T20:19:46.827631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Wordcloud** \n\n* A word cloud is a technique that visually represents the frequency of words in a text. Words that appear more frequently are displayed in larger fonts, while less common words are shown in smaller fonts","metadata":{}},{"cell_type":"code","source":"text= \" \".join(df[\"Review\"])","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:22:18.403983Z","iopub.status.busy":"2024-10-12T20:22:18.403483Z","iopub.status.idle":"2024-10-12T20:22:18.41235Z","shell.execute_reply":"2024-10-12T20:22:18.41091Z","shell.execute_reply.started":"2024-10-12T20:22:18.403936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud_result= WordCloud().generate(text)\n\nplt.imshow(wordcloud_result, interpolation=\"bilinear\")\n\nplt.axis(\"off\")\n\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:22:24.432601Z","iopub.status.busy":"2024-10-12T20:22:24.431464Z","iopub.status.idle":"2024-10-12T20:22:25.503347Z","shell.execute_reply":"2024-10-12T20:22:25.501763Z","shell.execute_reply.started":"2024-10-12T20:22:24.432535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wordcloud = WordCloud(max_font_size=50,\n\n                      max_words=100,\n\n                      background_color=\"white\").generate(text)\n\nplt.figure()\n\nplt.imshow(wordcloud, interpolation=\"bilinear\")\n\nplt.axis(\"off\")\n\nplt.show()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:22:29.624425Z","iopub.status.busy":"2024-10-12T20:22:29.623993Z","iopub.status.idle":"2024-10-12T20:22:30.519189Z","shell.execute_reply":"2024-10-12T20:22:30.517867Z","shell.execute_reply.started":"2024-10-12T20:22:29.624383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Sentiment Analysis:** \n\n* Sentiment analysis is a natural language processing (NLP) technique used to determine the emotional tone behind a series of words. It involves classifying text as positive, negative, or neutral based on the sentiments expressed within. This analysis helps in understanding opinions, attitudes, and emotions towards a product, service, or topic, and is commonly used in social media monitoring, customer feedback, and market research.\n\n","metadata":{}},{"cell_type":"code","source":"nltk.download(\"vader_lexicon\") ","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:25:32.201056Z","iopub.status.busy":"2024-10-12T20:25:32.200554Z","iopub.status.idle":"2024-10-12T20:25:32.213262Z","shell.execute_reply":"2024-10-12T20:25:32.21165Z","shell.execute_reply.started":"2024-10-12T20:25:32.201009Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sia= SentimentIntensityAnalyzer()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:25:36.513351Z","iopub.status.busy":"2024-10-12T20:25:36.512892Z","iopub.status.idle":"2024-10-12T20:25:36.536146Z","shell.execute_reply":"2024-10-12T20:25:36.53484Z","shell.execute_reply.started":"2024-10-12T20:25:36.513284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sia.polarity_scores(\"The story is excellent\")\n\n#The line essentially tries to determine the sentiment of the text: whether it is positive or negative.\n\n#In this context, a compound score greater than 0 typically indicates a positive sentiment.","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:58:41.484631Z","iopub.status.busy":"2024-10-12T20:58:41.484204Z","iopub.status.idle":"2024-10-12T20:58:41.492619Z","shell.execute_reply":"2024-10-12T20:58:41.491232Z","shell.execute_reply.started":"2024-10-12T20:58:41.484593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Review\"][0:10].apply(lambda x : sia.polarity_scores(x))","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:26:35.097603Z","iopub.status.busy":"2024-10-12T20:26:35.097132Z","iopub.status.idle":"2024-10-12T20:26:35.117345Z","shell.execute_reply":"2024-10-12T20:26:35.115465Z","shell.execute_reply.started":"2024-10-12T20:26:35.097555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"Review\"][0:10].apply(lambda x : sia.polarity_scores(x)[\"compound\"]).sort_values(ascending=False)","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:26:49.60529Z","iopub.status.busy":"2024-10-12T20:26:49.604817Z","iopub.status.idle":"2024-10-12T20:26:49.618861Z","shell.execute_reply":"2024-10-12T20:26:49.617608Z","shell.execute_reply.started":"2024-10-12T20:26:49.605246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# new column is maded using compound score \ndf[\"compound_score\"]=df[\"Review\"].apply(lambda x : sia.polarity_scores(x)[\"compound\"])","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:27:01.460621Z","iopub.status.busy":"2024-10-12T20:27:01.459819Z","iopub.status.idle":"2024-10-12T20:27:02.893692Z","shell.execute_reply":"2024-10-12T20:27:02.89249Z","shell.execute_reply.started":"2024-10-12T20:27:01.460572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# if compound score less than 0 this means this review is negative otherwise positive \ndf[\"sentiment_label\"]=df[\"compound_score\"].apply(lambda x: \"pos\" if x > 0.0 else \"neg\")","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:27:05.500494Z","iopub.status.busy":"2024-10-12T20:27:05.500047Z","iopub.status.idle":"2024-10-12T20:27:05.510495Z","shell.execute_reply":"2024-10-12T20:27:05.509102Z","shell.execute_reply.started":"2024-10-12T20:27:05.500452Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# lets see is there a rpw that compound score is positive but given star is less than 3 \n# try to capture why this is happened. Maybe the user give the wrong star or review \ndf[(df[\"compound_score\"]> 0.6) & (df[\"Star\"]<3)].head(10)","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:27:11.616138Z","iopub.status.busy":"2024-10-12T20:27:11.615721Z","iopub.status.idle":"2024-10-12T20:27:11.635112Z","shell.execute_reply":"2024-10-12T20:27:11.63386Z","shell.execute_reply.started":"2024-10-12T20:27:11.616099Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df[\"sentiment_label\"].value_counts()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:30:04.456639Z","iopub.status.busy":"2024-10-12T20:30:04.456176Z","iopub.status.idle":"2024-10-12T20:30:04.467987Z","shell.execute_reply":"2024-10-12T20:30:04.466538Z","shell.execute_reply.started":"2024-10-12T20:30:04.456596Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.groupby(\"sentiment_label\")[\"Star\"].mean() \n# division of sentiment labels is reasonable","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:30:15.40563Z","iopub.status.busy":"2024-10-12T20:30:15.405031Z","iopub.status.idle":"2024-10-12T20:30:15.422698Z","shell.execute_reply":"2024-10-12T20:30:15.419802Z","shell.execute_reply.started":"2024-10-12T20:30:15.405572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Modelling**","metadata":{}},{"cell_type":"code","source":"# change the representation of target using labelencoder\nlb= LabelEncoder()\ndf[\"sentiment_label\"]= lb.fit_transform(df[\"sentiment_label\"])","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:33:19.749818Z","iopub.status.busy":"2024-10-12T20:33:19.7493Z","iopub.status.idle":"2024-10-12T20:33:19.760276Z","shell.execute_reply":"2024-10-12T20:33:19.758115Z","shell.execute_reply.started":"2024-10-12T20:33:19.749763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y= df[\"sentiment_label\"]\nX= df[\"Review\"]","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:33:58.943128Z","iopub.status.busy":"2024-10-12T20:33:58.942614Z","iopub.status.idle":"2024-10-12T20:33:58.949665Z","shell.execute_reply":"2024-10-12T20:33:58.948113Z","shell.execute_reply.started":"2024-10-12T20:33:58.943083Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, X_test, y_train, y_test= train_test_split(X, y, random_state=42, test_size=0.2)","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:34:26.768252Z","iopub.status.busy":"2024-10-12T20:34:26.767764Z","iopub.status.idle":"2024-10-12T20:34:26.78209Z","shell.execute_reply":"2024-10-12T20:34:26.779996Z","shell.execute_reply.started":"2024-10-12T20:34:26.768207Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**A crucial aspect of Natural Language Processing (NLP)** is transforming texts and words into numerical representations. This process, known as **word vectorization** enables algorithms to understand and process language in a mathematical format. Such transformations are vital for various applications, including sentiment analysis, text classification, and machine translation.\n\nit is essential to transform our data into a measurable format, enabling us to perform mathematical operations and build machine learning models. To achieve this, we create word vectors. Commonly used methods for generating these vectors include:\n\n* **Count Vectors:** Represents text data as the frequency of each word.\n\n* **TF-IDF (Term Frequency-Inverse Document Frequency):** Normalizes word frequencies to highlight important words in relation to the entire dataset.\n\n* **Word Embedding Methods:** Techniques like Word2Vec, GloVe, and BERT that capture semantic meaning by mapping words into continuous vector spaces.\n\nThese methods allow for a more nuanced understanding of language and improve the performance of various NLP tasks.","metadata":{}},{"cell_type":"markdown","source":"**CountVectorizer:**\n\n* This method converts a collection of text documents into a matrix of token counts. Each unique word is represented as a column, and the entries in the matrix represent the frequency of each word in the documents. It's a straightforward way to convert text data into a numerical format for analysis and modeling.","metadata":{}},{"cell_type":"code","source":"bow=CountVectorizer()\n\nX_train_bow=bow.fit_transform(X_train).toarray()\n\nX_test_bow=bow.transform(X_test).toarray()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T21:17:51.619118Z","iopub.status.busy":"2024-10-12T21:17:51.618658Z","iopub.status.idle":"2024-10-12T21:17:51.793432Z","shell.execute_reply":"2024-10-12T21:17:51.792305Z","shell.execute_reply.started":"2024-10-12T21:17:51.619076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bow.get_feature_names_out()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:59:16.08952Z","iopub.status.busy":"2024-10-12T20:59:16.089019Z","iopub.status.idle":"2024-10-12T20:59:16.099285Z","shell.execute_reply":"2024-10-12T20:59:16.097913Z","shell.execute_reply.started":"2024-10-12T20:59:16.089475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_bow[10:14]","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:59:17.905563Z","iopub.status.busy":"2024-10-12T20:59:17.905027Z","iopub.status.idle":"2024-10-12T20:59:17.914526Z","shell.execute_reply":"2024-10-12T20:59:17.913049Z","shell.execute_reply.started":"2024-10-12T20:59:17.905511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression using CountVectorize**","metadata":{}},{"cell_type":"code","source":"logreg = LogisticRegression()\n\nlogreg.fit(X_train_bow, y_train)\n\ny_pred = logreg.predict(X_test_bow)\n\naccuracy = accuracy_score(y_test, y_pred)\n\nprint(f\"Accuracy: {accuracy}\")\n\n\n# Cross-validation uygulama (5 katmanlı)\n\ncv_scores = cross_val_score(logreg, X_train_bow, y_train, cv=5, scoring='accuracy')\n\nprint(f\"Cross-validation scores: {cv_scores}\")\n\nprint(f\"Mean cross-validation score: {cv_scores.mean()}\")","metadata":{"execution":{"iopub.execute_input":"2024-10-12T21:10:33.398498Z","iopub.status.busy":"2024-10-12T21:10:33.396602Z","iopub.status.idle":"2024-10-12T21:10:36.939403Z","shell.execute_reply":"2024-10-12T21:10:36.938171Z","shell.execute_reply.started":"2024-10-12T21:10:33.398427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TF-IDF (Term Frequency-Inverse Document Frequency):**\n\n* This method transforms text data into a numerical representation by considering not just the frequency of words but also their importance across multiple documents. It calculates the term frequency (TF) of a word in a specific document and multiplies it by the inverse document frequency (IDF) to reduce the weight of common words and highlight rare ones. This helps capture the significance of terms in the context of the entire dataset, making it a popular choice for text analysis and information retrieval.","metadata":{}},{"cell_type":"code","source":"tfidf=TfidfVectorizer()\n\nX_train_tfidf=tfidf.fit_transform(X_train).toarray()\n\nX_test_tfidf=tfidf.transform(X_test).toarray()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:58:59.734187Z","iopub.status.busy":"2024-10-12T20:58:59.733756Z","iopub.status.idle":"2024-10-12T20:59:00.0041Z","shell.execute_reply":"2024-10-12T20:59:00.002961Z","shell.execute_reply.started":"2024-10-12T20:58:59.734146Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf.get_feature_names_out()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T20:59:02.079249Z","iopub.status.busy":"2024-10-12T20:59:02.078831Z","iopub.status.idle":"2024-10-12T20:59:02.089947Z","shell.execute_reply":"2024-10-12T20:59:02.08854Z","shell.execute_reply.started":"2024-10-12T20:59:02.079211Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression using TF-IDF**","metadata":{}},{"cell_type":"code","source":"# Words with TF-IDF\n\nlog_model_idf = LogisticRegression().fit(X_train_tfidf, y_train)\n\npred_val_idf= log_model_idf.predict(X_test_tfidf)\n\naccuracy = accuracy_score(y_test, pred_val_idf)\n\nprint(f\"Accuracy: {accuracy}\")\n\ncv_scores = cross_val_score(log_model_idf, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n\nprint(f\"Cross-validation scores: {cv_scores}\")\n\nprint(f\"Mean cross-validation score: {cv_scores.mean()}\")","metadata":{"execution":{"iopub.execute_input":"2024-10-12T21:00:05.797563Z","iopub.status.busy":"2024-10-12T21:00:05.797081Z","iopub.status.idle":"2024-10-12T21:00:08.165883Z","shell.execute_reply":"2024-10-12T21:00:08.164264Z","shell.execute_reply.started":"2024-10-12T21:00:05.797503Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_test, pred_val_idf))","metadata":{"execution":{"iopub.execute_input":"2024-10-12T21:01:28.240508Z","iopub.status.busy":"2024-10-12T21:01:28.240041Z","iopub.status.idle":"2024-10-12T21:01:28.263934Z","shell.execute_reply":"2024-10-12T21:01:28.262093Z","shell.execute_reply.started":"2024-10-12T21:01:28.240465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_confusion_matrix(y, y_pred):\n    acc = round(accuracy_score(y, y_pred), 2)\n    cm = confusion_matrix(y, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\".0f\")\n    plt.xlabel('y_pred')\n    plt.ylabel('y')\n    plt.title('Accuracy Score: {0}'.format(acc), size=10)\n    plt.show()\n\nplot_confusion_matrix(y_test, pred_val_idf)","metadata":{"execution":{"iopub.execute_input":"2024-10-12T21:02:00.379445Z","iopub.status.busy":"2024-10-12T21:02:00.379029Z","iopub.status.idle":"2024-10-12T21:02:00.675591Z","shell.execute_reply":"2024-10-12T21:02:00.674251Z","shell.execute_reply.started":"2024-10-12T21:02:00.379405Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**TF-IDF with Ngram**","metadata":{}},{"cell_type":"code","source":"tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2, 3)) \n\nX_train_tfidf_ngram=tf_idf_ngram_vectorizer.fit_transform(X_train).toarray()\n\nX_test_tfidf_ngram =tf_idf_ngram_vectorizer.transform(X_test).toarray() ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Logistic Regression using TF-IDF Ngram**","metadata":{}},{"cell_type":"code","source":"# Words with TF-IDF\nlog_model_idf_ngram = LogisticRegression().fit(X_train_tfidf_ngram, y_train)\n\npred_val_idf_ngram= log_model_idf_ngram.predict(X_test_tfidf_ngram)\n\naccuracy = accuracy_score(y_test, pred_val_idf_ngram)\n\nprint(f\"Accuracy: {accuracy}\")\n\ncv_scores = cross_val_score(log_model_idf_ngram, X_train_tfidf_ngram, y_train, cv=5, scoring='accuracy')\n\nprint(f\"Cross-validation scores: {cv_scores}\")\n\nprint(f\"Mean cross-validation score: {cv_scores.mean()}\")","metadata":{"execution":{"iopub.execute_input":"2024-10-12T21:02:34.713228Z","iopub.status.busy":"2024-10-12T21:02:34.71273Z","iopub.status.idle":"2024-10-12T21:03:22.986563Z","shell.execute_reply":"2024-10-12T21:03:22.983933Z","shell.execute_reply.started":"2024-10-12T21:02:34.71318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**lets Randomly select one review and look at the label**","metadata":{}},{"cell_type":"code","source":"## Rastgele bir yorumun seçilmesi \n\nrandom_comment= pd.Series.sample(X_test)\n\nrandom_comment","metadata":{"execution":{"iopub.execute_input":"2024-10-12T21:13:12.085004Z","iopub.status.busy":"2024-10-12T21:13:12.08452Z","iopub.status.idle":"2024-10-12T21:13:12.100512Z","shell.execute_reply":"2024-10-12T21:13:12.099166Z","shell.execute_reply.started":"2024-10-12T21:13:12.084957Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random=bow.transform(random_comment).toarray()","metadata":{"execution":{"iopub.execute_input":"2024-10-12T21:18:16.403824Z","iopub.status.busy":"2024-10-12T21:18:16.403359Z","iopub.status.idle":"2024-10-12T21:18:16.410165Z","shell.execute_reply":"2024-10-12T21:18:16.408583Z","shell.execute_reply.started":"2024-10-12T21:18:16.40378Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"logreg.predict(random)","metadata":{"execution":{"iopub.execute_input":"2024-10-12T21:18:18.874033Z","iopub.status.busy":"2024-10-12T21:18:18.873537Z","iopub.status.idle":"2024-10-12T21:18:18.884428Z","shell.execute_reply":"2024-10-12T21:18:18.882533Z","shell.execute_reply.started":"2024-10-12T21:18:18.873986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Random Forest** ","metadata":{}},{"cell_type":"code","source":"# Random Forest Modeli\n\nrf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n\ncv_scores__rf_tfıdf = cross_val_score(rf_model, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n\nprint(f\" tfıdf Mean CV Accuracy: {cv_scores__rf_tfıdf.mean()}\")\n\ncv_scores_rf_bow = cross_val_score(rf_model, X_train_bow, y_train, cv=5, scoring='accuracy')\n\nprint(f\" bow Mean CV Accuracy: {cv_scores_rf_bow.mean()}\") \n\n\n\n# Cross-validation uygulama (5 katmanlı)\n\n#cv_scores_rf_ngram = cross_val_score(rf_model, X_train_tfidf_ngram, y_train, cv=5, scoring='accuracy')\n#print(f\" ngram Mean CV Accuracy: {cv_scores_rf_ngram.mean()}\")   ","metadata":{"execution":{"iopub.execute_input":"2024-10-12T21:27:17.139066Z","iopub.status.busy":"2024-10-12T21:27:17.138633Z","iopub.status.idle":"2024-10-12T21:28:05.31564Z","shell.execute_reply":"2024-10-12T21:28:05.314379Z","shell.execute_reply.started":"2024-10-12T21:27:17.139023Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\n# Retrieve the mean accuracy scores for each model\nmean_scores = [cv_scores__rf_tfıdf.mean(), cv_scores_rf_bow.mean()]\n\nmodel_names = ['TF-IDF', 'BoW']\n\n# Create a bar chart\nfig = go.Figure([go.Bar(x=model_names, y=mean_scores, text=mean_scores, textposition='auto')])\n\n# Customize the chart (title and axis labels)\nfig.update_layout(\n    title=\"Cross-Validation Mean Accuracy Scores for Different Vectorizations\",\n    xaxis_title=\"Vectorization Techniques\",\n    yaxis_title=\"Mean Accuracy\",\n    yaxis=dict(range=[0, 1]),  # Set the Y-axis range from 0 to 1\n    plot_bgcolor='white',  # Set the background color of the chart to white\n    paper_bgcolor='white',  # Set the overall background color to white\n    template=\"plotly\"  # Default template\n)\n\n# Show numerical values on hover\nfig.update_traces(marker_color='lightgreen', marker_line_color='darkgreen', marker_line_width=1.5, opacity=0.8)\n\n# Display the chart\nfig.show()\n","metadata":{"execution":{"iopub.execute_input":"2024-10-12T21:28:52.552355Z","iopub.status.busy":"2024-10-12T21:28:52.551864Z","iopub.status.idle":"2024-10-12T21:28:54.661657Z","shell.execute_reply":"2024-10-12T21:28:54.660594Z","shell.execute_reply.started":"2024-10-12T21:28:52.55231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Result** \n\n- Based on the results, we can see that the logistic regression model built with CountVectorizer outperformed the models using TF-IDF and TF-IDF with N-grams. Additionally, when RandomForest was applied to the same vectorization methods, we observed a noticeable improvement in performance, indicating that the model's accuracy increased with this more complex algorithm.","metadata":{}},{"cell_type":"markdown","source":"**Conclusion**\n\n- Hello again! It was really enjoyable for me to complete this work. I tried to concretize my analyzes on NLP Sentiment Analysis with this Amazon data set. If you liked the work I did on this subject, I am waiting for your comments. If you have any bugs or improvement suggestions, please share. Also, if you have suggestions or topics you would like to include in my future projects, let me know. I look forward to your contributions and thank you! 🚀📈","metadata":{}}]}